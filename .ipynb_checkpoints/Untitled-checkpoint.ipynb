{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Entre a word, or (e) to exit: نزول\n",
      "\n",
      "                             [1]\n",
      "\tالإسرائيلية تستعد لتستضيف مئات آلاف اليهود في عيد \u001b[36mنزول\u001b[0m التوراة ولم يكن المكان الضيق بين حائط ------------------------------------------------------------------------------------------------------------------------\n",
      "                             [2]\n",
      "\tأسلم الناس ودخلوا في دين الله أفواجا بعد \u001b[36mنزول\u001b[0m تحريم المشركات ، ونزول النهي عن التمسك ------------------------------------------------------------------------------------------------------------------------\n",
      "                             [3]\n",
      "\tالعلاقة الزوجية التي كانت قبل \u001b[36mنزول\u001b[0m آية المهاجرات ، وكان اختلاف الدين موجودا ------------------------------------------------------------------------------------------------------------------------\n",
      "                             [4]\n",
      "\tفحاصل النظر في سبب \u001b[36mنزول\u001b[0m الآية وما احتف بها من حيثيات عدم ------------------------------------------------------------------------------------------------------------------------\n",
      "                             [5]\n",
      "\tإلى استقراء آفاق علوم ومعطيات عدة وإذا كان \u001b[36mنزول\u001b[0m إنسان على القمر في فترة من الفترات ------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "   جذر كلمة نزول هو : \u001b[32mنزل\u001b[0m\n",
      "   The root of  نزول is : \u001b[32mنزل\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "   أشكال أخرى لكلمات من نفس الجذر \n",
      " Other forms of the words : \n",
      "  [1] تنزل                  [2] ننزل                  [3] بمنزله                [4] نزلوا               \n",
      "  [5] لمنزلي                [6] والمنزلة              [7] والمنازل              [8] منزلهم              \n",
      "  [9] لنزول                 [10] نزولنا                [11] لنزولك                [12] بمنزلتك             \n",
      "  [13] ونننزل                [14] أنزل                  [15] منزلين                [16] بمنازلهم            \n",
      "  [17] والمنزل               [18] للمنازل               [19] أنازلهم               [20] نزال                \n",
      "  [21] نتنازل                [22] بنزول                 [23] بالمنازل              [24] وينزل               \n",
      "  [25] نزيلة                 [26] للنزيل                [27] ونزولهم               [28] نزلت                \n",
      "  [29] بالنزول               [30] تنزيل                 [31] منزل                  [32] تنزيله              \n",
      "  [33] بمنزلتهم              [34] منزلنا                [35] النزل                 [36] ونزل                \n",
      "  [37] نزل                   [38] منازله                [39] نزوله                 [40] بمنزلة              \n",
      "  [41] نزيلا                 [42] نازل                  [43] ينزل                  [44] نزلات               \n",
      "  [45] تنازلت                [46] تتنازل                [47] يتنزل                 [48] تنازلها             \n",
      "  [49] المنزلة               [50] بمنزل                 [51] تنازل                 [52] فينزل               \n",
      "  [53] منازلهم               [54] لمنزل                 [55] يتنازلون              [56] لمنزله              \n",
      "  [57] منازلها               [58] المنازل               [59] نازلة                 [60] منزلة               \n",
      "  [61] وتنزل                 [62] ينزلها                [63] التنازلات             [64] النازل              \n",
      "  [65] لينزل                 [66] وننزل                 [67] للمنزل                [68] بمنزلته             \n",
      "  [69] أنزلنا                [70] منازلهن               [71] ومنازل                [72] بالتنازلات          \n",
      "  [73] ونزلنا                [74] بالتنازل              [75] لنزولها               [76] التنزيل             \n",
      "  [77] التنازل               [78] انزل                  [79] نزلنا                 [80] تنازلات             \n",
      "  [81] ونزول                 [82] نزيل                  [83] لمنازل                [84] ونزلت               \n",
      "  [85] منازل                 [86] نازلا                 [87] ومنزلته               [88] منزلكم              \n",
      "  [89] ومنزلك                [90] سينزل                 [91] نزول                  [92] نزلها               \n",
      "  [93] يتنازل                [94] تتنازلين              [95] منازلنا               [96] النزول              \n",
      "  [97] منزلها                [98] التنزيلات             [99] ونوازل                [100] بمنازلها            \n",
      "  [101] المنزل              "
     ]
    }
   ],
   "source": [
    "# Reading files libraries\n",
    "from os import listdir \n",
    "from os.path import isdir, join \n",
    "import xml.etree.ElementTree as et # To read and manipulate XML files and Parsing XML Data\n",
    "import re # For Regular expression\n",
    "import json # To save files in json formate\n",
    "# For Natural language processing (NLP)\n",
    "from nltk.tokenize import WordPunctTokenizer # To break sentences into words \n",
    "from nltk.tokenize import sent_tokenize # To break paragraphs into sentences \n",
    "from nltk.stem.isri import ISRIStemmer # To get the root of  words\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "\n",
    "doc_list = [] # It will contain list of all files that we have in our database\n",
    "word_occurances = {} #our dic which contains keys (words) and values (list of sentences that have the words).\n",
    "forms_dict = {} # it will contain keys (words) and values (list of words that are from the same root in our files).\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                      ّ    | # Tashdid\n",
    "                      َّ    | # Fatha\n",
    "                      ً    | # Tanwin Fath\n",
    "                      ُ    | # Damma\n",
    "                      ٌ    | # Tanwin Damm\n",
    "                      ِ    | # Kasra\n",
    "                      ٍ    | # Tanwin Kasr\n",
    "                      ْ    | # Sukun\n",
    "                     ـ     # Tatwil/Kashida\n",
    "                       \"\"\", re.VERBOSE) # To define the Arabic diacritics \n",
    "\n",
    "\n",
    "def build_all_paths_list(dir_name): # To build list of paths\n",
    "    paths_list = []\n",
    "    \n",
    "    all_sub_dir = listdir(dir_name) # To get list of all sub directory \n",
    "    \n",
    "    for sub_dir in all_sub_dir: # To get the directories that are inside the sub directories\n",
    "        base_path = join(dir_name, sub_dir) # integration function to access the files\n",
    "        if isdir(base_path): # To check if it directory or not\n",
    "           for file_name in listdir(base_path): # To get the files' names inside the directory\n",
    "             if not file_name.startswith(\".\"):\n",
    "              paths_list.append(join(base_path, file_name))\n",
    "    return paths_list\n",
    "\n",
    "\n",
    "def parse_XML(xml_file):\n",
    "    doc = {} # To save the extracted information from the files\n",
    "    try:\n",
    "         xtree = et.parse(xml_file)\n",
    "         xroot = xtree.getroot() #To get the first tag in our root\n",
    "    \n",
    "         doc[\"name\"] = xroot.find(\"teiHeader\").attrib.get(\"id\")\n",
    "         #doc[\"domain\"] = re.split(r\"[/\\\\]\", xml_file)[-2]\n",
    "         \n",
    "    \n",
    "         \n",
    "         doc[\"sentences\"] = [] #To get a list of  all sentences in each file\n",
    "         for p in xroot.findall(\".//body//p\"):\n",
    "             if p.text != None: #  To make sure that P is not empty \n",
    "               text = p.text.strip() # To remove the extra spaces\n",
    "               text = re.sub(arabic_diacritics, \"\", text) # To exchange arabic_diacritics into empty (Delete)\n",
    "               text = re.sub(r\"(\\s)\\1+\", r\"\\1\", text)  # To remove all repeated spaces\n",
    "               sentences = sent_tokenize(text.replace(\"؟\", \"?\")) #To break the paragraph into sentences. and convert the question mark from arabi\n",
    "               for i, sentence in enumerate (sentences): \n",
    "                 sentences[i] =re.sub(r'[^\\u0600-\\u06FF\\s]', '', sentence).strip() #To exchange anything is not aranic into empty (Delete)\n",
    "               doc[\"sentences\"].extend(sentences)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(xml_file)\n",
    "\n",
    "\n",
    "    return doc\n",
    "\n",
    "def build_all_docs_list(dir_name): # To build our doc list\n",
    "    paths = build_all_paths_list(dir_name) # Call build_all_paths_list fuction to build list of paths\n",
    "\n",
    "    for path in paths:\n",
    "      if path.endswith(\".xml\"): # To access to XML files \n",
    "        doc = parse_XML(path) # Call parse_XML function.\n",
    "        doc_list.append(doc)\n",
    "\n",
    "def extract_words():\n",
    "\n",
    "  st = ISRIStemmer() # To get the root of the words\n",
    "  tk = WordPunctTokenizer() # To break sentences into words \n",
    "\n",
    "  for doc in doc_list: \n",
    "      for sentence in doc[\"sentences\"]: # To access to the sentences in each doc\n",
    "          words = {word for word in tk.tokenize (sentence) if word.isalpha()} # To breake the sent. into words, if the word is Alphabet. \n",
    "          \n",
    "\n",
    "          \n",
    "          for word in words: \n",
    "              if st.stem(word) in forms_dict: # To get and check if the root of the word is in the directory.\n",
    "                  forms_dict[st.stem(word)].add(word) # Add the word to the list of words that are from the same root\n",
    "              else:\n",
    "                  forms_dict[st.stem(word)] = {word} # Add the word (root) to the new list \n",
    "                  \n",
    "              if word in word_occurances: # To check if the word is from the words that are in the sentences\n",
    "                   word_occurances[word].append((sentence))  # Add the sentence that include the word to the list\n",
    "              else:\n",
    "                   word_occurances[word] = [(sentence)] # Add the sentence that include the word to new list\n",
    "                       \n",
    "  return word_occurances\n",
    "\n",
    "  \n",
    "def save_all_wordsـjson(): #Save list of words in JSON formate\n",
    "   with open (\"words.json\", \"w\", encoding='utf8') as file:\n",
    "    json.dump(list(word_occurances.keys()), file, ensure_ascii=False)\n",
    "    \n",
    "    \n",
    "\n",
    "def save_all_words_txt(): #Save list of words in TXT formate\n",
    "   with open (\"words.txt\", \"w\") as file:\n",
    "    for word in word_occurances:\n",
    "      file.write(word + \"\\n\")\n",
    "      \n",
    "      \n",
    "\n",
    "def fine_sentences (word):\n",
    "   st = ISRIStemmer()\n",
    "   word = st.stem(word)\n",
    "   \n",
    "   sentences = set() \n",
    "   if word in word_occurances: \n",
    "      for sent in word_occurances[word]:\n",
    "          sentences.add(sent)\n",
    "          print(sent)\n",
    "          print(\"--\" * 100)\n",
    "\n",
    "   return list(sentences)\n",
    "\n",
    "\n",
    "def find_part_of_sentence(word):\n",
    "   st = ISRIStemmer() # To get the root of the words\n",
    "   tk = WordPunctTokenizer() # To break sentences into words \n",
    "\n",
    "   \n",
    "   quots = set() # Creating a set of sentences that include the word that entered.\n",
    "   if word in word_occurances: # To of the word is on the corpus \n",
    "     for i, oocur in enumerate (word_occurances[word]): # for each sentence in word_occurances dict\n",
    "         sentence = oocur  \n",
    "         words_bag = [token for token in tk.tokenize(sentence)] # To convert the sentences to words, in order to find its index\n",
    "         idx = words_bag.index(word) # To reture the index of the word.\n",
    "         print(\"                             \"f\"[{i+1}]\", end=\"\") # Print the onther form of the words\n",
    "        # print(\"The domain of the concordance is : \", colored(domain, \"magenta\"))\n",
    "    \n",
    "         print()\n",
    "         print(end=\"\\t\")\n",
    "         quot = \"\"\n",
    "\n",
    "         if idx>= 8:\n",
    "             \n",
    "             for token in tk.tokenize(sentence)[idx - 8: idx + 8]:\n",
    "                \n",
    "                 if token == word:\n",
    "                      print(colored(token, \"cyan\"), end=\" \")\n",
    "                 else:\n",
    "                     print (token, end=\" \")\n",
    "                 quot += token + \" \"\n",
    "         else:\n",
    "             for token in tk.tokenize(sentence)[: idx + 8]:\n",
    "                 if token == word:\n",
    "                     print(colored(token, \"cyan\"), end=\" \")\n",
    "                 else:\n",
    "                     print (token, end=\" \")\n",
    "                     \n",
    "                 quot += token + \" \"\n",
    "                     \n",
    "                     \n",
    "         quots.add((quot))\n",
    "         print()\n",
    "         print(\"--\" * 50)\n",
    "     print_other_forms(word) # To call the root fuction\n",
    "         \n",
    "   else:\n",
    "         print(\"   لا توجد أمثلة لهذه الكلمه \",  \" \\n There is on concordances for this word\") \n",
    "         \n",
    "   \n",
    "   return list(quots)  \n",
    "\n",
    "\n",
    "\n",
    "def print_other_forms(word):\n",
    "    st = ISRIStemmer()\n",
    "    \n",
    "    \n",
    "    root = st.stem(word) \n",
    "   # print (\"The root of \" + word + \" is\"+ root)\n",
    "    print()\n",
    "    print (\"  \",\"جذر كلمة \" + word+ \" هو : \"+ colored(root, \"green\"))\n",
    "    print (\"  \",\"The root of  \" + word+ \" is : \"+ colored(root, \"green\"))\n",
    "    \n",
    "    if root in forms_dict:\n",
    "        print(\"\\n\\n\\n   أشكال أخرى لكلمات من نفس الجذر\", \"\\n Other forms of the words : \", )\n",
    "        \n",
    "        for i, form in enumerate(forms_dict[root]): # for each root of words in forms_dict\n",
    "            print(\"  \"f\"[{i+1}] {form:20s}\", end=\"\") # Print the onther form of the words\n",
    "            if (i+1) % 4 == 0:\n",
    "                print()\n",
    "        return [root] + list (forms_dict[root])\n",
    "    else:\n",
    "         print (\"\\n There is no other forms\")\n",
    "         \n",
    "     \n",
    "\n",
    "build_all_docs_list(\"resources\") # To call the function and give the folder that have the files\n",
    "extract_words() # To call the function to extract the words from doc list\n",
    "save_all_wordsـjson()\n",
    "save_all_words_txt()\n",
    "\n",
    "\n",
    "while True: # Loop\n",
    " word = input (\" Entre a word, or (e) to exit: \")\n",
    " print()\n",
    " if word == 'e': # break the loop.\n",
    "     print (\"\\n Thank you .\")\n",
    "     break\n",
    "     \n",
    " result = find_part_of_sentence(word)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
